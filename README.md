# donasktello_microservices
## Д/3 №11
- Репозиторий настроен на интеграцию с travis-ci
- Установлен Docker
- Запущен первый контейнер 'Hello, world'
- Создан и закоммитен первый образ на основе запущенного контейнера

## Д/3 №12
- Создал GCP проект docker
- Создан инстанс docker-host через docker-machine
- Собран билд реддит проекта на докер хост
- Контейнер запущен на докер хосте и прописано правило файрвола для доступа
- Образ билда запушен на докер хаб
- Контейнер из образа запущен и проверен локально

## Д/З №13
- Скачан и распакован архив microservices
- Поочерёдно созданы Dockerfile для каждого из 3 микросервисов(комменты, посты и ui)
- Сбилдены образы для всех 3 микросервисов
- Создана сеть для приложения
- Микросервисы соединены в одну общую сеть
- Создан volume для mongo_db и подключен при запуске контейнера монго

## Д/З №14
- Поработали с none сетевым драйвером на примере тестового контейнера
- Поработали с host сетевым драйвером на примере тестового контейнера
- Поработали с bridge сетевым драйвером на примере тестового контейнера
- Запустили реддит проект в 2 бридж сетях
- Посмотрели на конфигурацию iptables на docker-host
- Переписали проект под использование docker-compose
    ###Как образуется базовое имя проекта?
    Указать -p, --project-name NAME

## Д/З №15
- Создали виртуальную машину для gitlab-ci
- Установили докер на вм gitlab-ci
- Развернули gitlab-ci через docker-compose
- Создали рабочую группу и проект
- Добавили remote для заливки проекта
- Создали и настроили .gitlab-ci.yml
- Запустили контейнер для runner, указав креды доступа с нашего проекта
- Проставили галочку для запуска tagged jobs в настройках runner
- Залили reddit проект в репозиторий и изменили .gitlab-ci.yml для теста reddit
- Добавили файл simpletest для теста reddit и добавили гем rack-test для тестов

## Д/3 №16
- Создали новый проект example2 в gitlab-ci контейнере
- Добавили в качестве git репозитория gitlab2
- Расшарили runner на оба проекта, поменяв при этом .toml файл указав новый эфемерный ip
- Добавили dev environment
- Определили stage и prod окружения с ограничением по ручному деплою
- Сделали условие на наличие тега для деплоя на stage и prod окружения
- Создали возможность динамического окружения

## Д/3 №17
- Создали правила firewall для Prometheus и Puma (--allow tcp:9090 --allow tcp:9292)
- Создадим Docker хост в GCE и настроим локальное окружение на работу с ним
- Запускаем Prometheus из готового контейнера
- Переогранизовали предыдущую структуру и перенесли все докер файлы в единую docker директорию
- Копируем конфигурацию из готового контейнера Prometheus в наш контейнер
- Определяем конфигурационный файл для сбора метрик с наших микросервисов
- Билдим образ Prometheus контейнера на итоге собранных данных
- Сбилдили все образы наших микросервисов
- Отрефакторили docker-compose.yml(Убрали билд и актуализировали обращение к image, дописали контейнер Prometheus)
- Проверили docker-compose микросервисов в связке с Prometheus
- Остановили работу одного из микросервисов и убедились в том, что healthchecks возвращают down состояние микросервиса
- Познакомились с Exporters(Программа, которая делает метрики доступными для сбора Prometheus. Используется когда нельзя поменять код
    приложения. Примеры: PostgreSQL, RabbitMQ, Nginx, Node exporter, cAdvisor)
- Воспользовались Node экспортер и заодно допишем его в наш docker-compose. Также добавили в конфигурацию Prometheus ещё один 
    ендпойнт для сбора метрик(node-exporter:9100)
- Пересобрали образ Prometheus с новой конфигурацией и пересобрали наш docker-compose
- Запушили все наши образы микросервисов, Prometheus на DockerHub

## Д/З №18
- Создали Docker хост в GCE и настроили локальное окружение для работы с ним
- Разбили docker-compose на сборку приложения и сервисов мониторинга
- Добавили сервис cAdvisor в сборку по мониторингу для отслеживания потребления ресурсов приложения
- Добавили Grafana для визуализизации данных из Prometheus
- Добавили Data Source для получения данных из Prometheus в Grafana и кастомизировали её дашборд
- Добавили в конфигурацию Prometheus ендпойнт для стягивания данных с post микросервиса
- Добавили на дашборд графики(кол-ва запросов, гистограмму ошибочных запросов и гистограмму запросов по времени)
- Создали ещё один дашборд с графиками обращений на микросервис постов и комментариев
- Добавили alert из Prometheus в слак канал, если один из микросервисов падает

## Д/З №19
- Обновили код микросервисов и сбилдили для каждого микросервиса новый образ docker
- Создали Docker хост в GCE и настроили локальное окружение на работу с ним
- Рассмотрели стек ELK и EFK, остановившись на EFK
- Создали отдельный docker-compose файл для EFK связки контейнеров
- Билд для Fluentd мы собрали самостоятельно, при этом прописав fluentd.conf для ротации логов
- Определили драйвер fluentd для логирования для сервиса post внутри контейнера
- Создали правило фаервола для Kibana и настроили Kibana для мапинга индекса по паттерну
- Увидели нужную нам информацию логов из post сервиса в Kibana
- Добавили во fluent.conf фильтр для парсинга json логов в нужном нам формате
- Пересобрали и перезапустил образ fluentd
- Определили драйвер для логирования ui сервиса
- Применили парсинг для ui сервиса в fluent.conf
- Обновили образы и перезапустили сервисы

## Д/З №20
- Создали файлы манифесты для микросервисов
- Прошли туториал Kubernetes The Hard way
- Проверили, что файлы манифесты отрабатывают на созданном класетре kubernetes

## Д/З №21
- Установили kubectl для работы с Kubernetes API(все, что делает kubectl, можно сделать с помощью HTTP-запросов к API k8s)
- Установили minikube для разворачивания локальной инсталляции Kubernetes
- В качестве гипервизора для minikube использовали VirtualBox
- Развернули наш Minikube-кластер, при этом был автоматически настроен kubectl
- Создали и запустили Deployment для ui компонента
- Сделали форвардинг для ui на порт 9292
- Создали и запустили deployment для компонентов post и comment
- Создали deployment для mongo, указав ему стандартный volume для хранения информации
- Для связи компонентов между собой и внешним миром на каждый компонент сделали манифест service(абстракция,
  которая определяет набор POD-ов (Endpoints) и способ доступа к ним)
- Создали service для post и comment
- Создали service для mongodb, т.к. она используется и остальными сервисами
- Создали дополнительные service для связи post и comment с mongo
- Создали service для ui компонента, чтобы предоставить доступ к нему снаружи, указав тип сервиса NodePort, по умолчанию все ClusterIP
- Проверили стандартные аддоны minikube
- Посмотрели стандартный аддон dashboard от minikube
- Отделили среду разработки от всего отдельного кластера, создав namespace dev
- Запустили приложение в dev namespace `kubectl apply -n dev -f .`
- Добавили информацию об окружении в ui deployment
- Создали Kubernetes cluster в GCE
- Подключились к GKE для запуска нашего приложения `gcloud container clusters get-credentials cluster-1 --zone us-central1-a --project docker-182408` 
- Создали dev namespace в current context(GKE) `kubectl apply -f ./kubernetes/reddit/dev-namespace.yml`
- Задеплоили все компоненты приложения в dev namespace `kubectl apply -f ./kubernetes/reddit/ -n dev`
- Открыли reddit для внешнего мира(создав правило брандмауэра)
- Настроили `Целевые экземпляры - все экземпляры в сети` `Диапазоны IP-адресов источников  - 0.0.0.0/0`
    `Протоколы и порты - Указанные протоколы и порты tcp:30000-32767`
- Выбрали свободную ноду из кластера `kubectl get nodes -o wide `
- Нашли порт публикации service ui ` kubectl describe service ui -n dev | grep NodePort `
- Проверили работу reddit по адресу `http://<node-ip>:<NodePort>`
- Также в GKE проверили Dashboard для кластера ` kubectl proxy`
- Заходим по адресу http://localhost:8001/ui
- У dashboard не хватает прав, назначаем их `kubectl create clusterrolebinding kubernetes-dashboard  --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard`
